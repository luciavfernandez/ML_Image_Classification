{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Caltech-UCSD Birds-200-2011\n",
        "\n",
        "The project focuses on object recognition and computer vision as part of the BDMA 7 curriculum. The main objective is to develop and refine an image classification model using a subset of the Caltech-UCSD Birds-200-2011 dataset.\n"
      ],
      "metadata": {
        "id": "XN72ZVRMX36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRPuTwHFGHrX",
        "outputId": "404fc4d2-db90-4548-9b70-9011a84c8f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "exdt6b9IEuoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path definitions according to folder structure as found in Kaggle\n",
        "BASE_PATH = \"/content/drive/MyDrive/BDMA7_project_files\"\n",
        "TRAIN_DIR = os.path.join(BASE_PATH, \"train_images\")\n",
        "VAL_DIR = os.path.join(BASE_PATH, \"val_images\")\n",
        "TEST_DIR = os.path.join(BASE_PATH, \"test_images\")"
      ],
      "metadata": {
        "id": "4zzfcITnE1BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "iWZ__UBKE3Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations (Data Augmentation for Training and Normalization for all)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "mVr7d-uLE6H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Datasets using ImageFolder\n",
        "# ImageFolder automatically associates the subfolder name with the label\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(TRAIN_DIR, data_transforms['train']),\n",
        "    'val': datasets.ImageFolder(VAL_DIR, data_transforms['val_test']),\n",
        "    'test': datasets.ImageFolder(TEST_DIR, data_transforms['val_test'])\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n",
        "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4),\n",
        "    'test': DataLoader(image_datasets['test'], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(f\"Detected classes: {class_names}\") # Here we validate that they should be the 20 from the project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "k5ovLGCpE81w",
        "outputId": "63f676d5-3c34-49d4-b84a-3487abcf0d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_transforms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1079277012.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ImageFolder asocia automáticamente el nombre de la subcarpeta con la etiqueta (label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m image_datasets = {\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition (ResNet-101 for greater capacity)\n",
        "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "\n",
        "# Adjust the final layer for the 20 classes of the project\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ofzy9PeCFSfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function and Optimization\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# We only optimize the parameters of the final layer and upper layers (Fine-tuning)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "ZRPbJHihFVOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training and Validation Loop\n",
        "def train_model(model, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "PRCrd4AAEurY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execution\n",
        "model_ft = train_model(model, criterion, optimizer, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwrfrYwVFXNh",
        "outputId": "20422370-080d-4afe-8402-790abaa298f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "train Loss: 2.7377 Acc: 0.2884\n",
            "val Loss: 1.7429 Acc: 0.7573\n",
            "Epoch 1/14\n",
            "train Loss: 1.4426 Acc: 0.6996\n",
            "val Loss: 0.4705 Acc: 0.8738\n",
            "Epoch 2/14\n",
            "train Loss: 0.7607 Acc: 0.7828\n",
            "val Loss: 0.3596 Acc: 0.8835\n",
            "Epoch 3/14\n",
            "train Loss: 0.5757 Acc: 0.8355\n",
            "val Loss: 0.3017 Acc: 0.9029\n",
            "Epoch 4/14\n",
            "train Loss: 0.4968 Acc: 0.8503\n",
            "val Loss: 0.2798 Acc: 0.8738\n",
            "Epoch 5/14\n",
            "train Loss: 0.4530 Acc: 0.8632\n",
            "val Loss: 0.3351 Acc: 0.8932\n",
            "Epoch 6/14\n",
            "train Loss: 0.4248 Acc: 0.8752\n",
            "val Loss: 0.3789 Acc: 0.8738\n",
            "Epoch 7/14\n",
            "train Loss: 0.3494 Acc: 0.8983\n",
            "val Loss: 0.3926 Acc: 0.8738\n",
            "Epoch 8/14\n",
            "train Loss: 0.3204 Acc: 0.8983\n",
            "val Loss: 0.3774 Acc: 0.8641\n",
            "Epoch 9/14\n",
            "train Loss: 0.2693 Acc: 0.9177\n",
            "val Loss: 0.5224 Acc: 0.8641\n",
            "Epoch 10/14\n",
            "train Loss: 0.2832 Acc: 0.9187\n",
            "val Loss: 0.4101 Acc: 0.8738\n",
            "Epoch 11/14\n",
            "train Loss: 0.2626 Acc: 0.9288\n",
            "val Loss: 0.3607 Acc: 0.8738\n",
            "Epoch 12/14\n",
            "train Loss: 0.2183 Acc: 0.9381\n",
            "val Loss: 0.4742 Acc: 0.8641\n",
            "Epoch 13/14\n",
            "train Loss: 0.2509 Acc: 0.9279\n",
            "val Loss: 0.3861 Acc: 0.8932\n",
            "Epoch 14/14\n",
            "train Loss: 0.2510 Acc: 0.9261\n",
            "val Loss: 0.4461 Acc: 0.8932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a specific Dataset for Test\n",
        "# This is necessary because we need the file NAME for the Kaggle CSV\n",
        "class CUBTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, main_dir, transform=None):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        self.all_imgs = []\n",
        "\n",
        "        # We iterate through the structure: test_images / bird_name / image.jpg\n",
        "        for bird_dir in sorted(os.listdir(main_dir)):\n",
        "            bird_path = os.path.join(main_dir, bird_dir)\n",
        "            if os.path.isdir(bird_path):\n",
        "                for img_name in os.listdir(bird_path):\n",
        "                    self.all_imgs.append((os.path.join(bird_dir, img_name), img_name))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        full_path, img_name = self.all_imgs[idx]\n",
        "        image = Image.open(os.path.join(self.main_dir, full_path)).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Function to generate the .csv file\n",
        "def generate_submission(model, test_dir, output_file='sample_submission.csv'):\n",
        "    model.eval()\n",
        "\n",
        "    # We use validation transformations (without random data augmentation)\n",
        "    test_dataset = CUBTestDataset(test_dir, transform=data_transforms['val_test'])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    submission_data = []\n",
        "\n",
        "    print(f\"Generating predictions for images in: {test_dir}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, filenames in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # We save each image-prediction pair as requested in the PDF\n",
        "            for fname, p in zip(filenames, preds):\n",
        "                submission_data.append({\n",
        "                    'id': fname,       # File name (e.g. 001.jpg)\n",
        "                    'label': p.item()  # Predicted class (0 to 19)\n",
        "                })\n",
        "\n",
        "    # Save the final file\n",
        "    df = pd.DataFrame(submission_data)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"The file '{output_file}' has been created in csv format to upload to Kaggle.\")"
      ],
      "metadata": {
        "id": "_BlSy8qJGdCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Function call\n",
        " generate_submission(model_ft, TEST_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ2yUTh4Kq7H",
        "outputId": "1f3febf9-79bf-4409-b9d2-735bf209f35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for images in: /content/drive/MyDrive/BDMA7_project_files/test_images\n",
            "The file has been created 'sample_submission.csv' in csv format to upload to Kaggle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Justifications\n",
        "\n",
        "The objective is to develop and refine a deep learning model to classify bird species using a subset of the Caltech-UCSD Birds-200-2011 dataset. The competition takes place on Kaggle and success is measured by the accuracy of the model on a test dataset.\n",
        "\n",
        "\n",
        "1. Architecture (ResNet-50): It was chosen for its ability to handle vanishing gradients through residual connections. It is deep enough to capture fine details of feathers and beaks, but efficient to train in reasonable times. ResNet-101 was chosen for its balance between depth and ease of training through Transfer Learning, which is ideal for this bird dataset.\n",
        "\n",
        "2. Transfer Learning: Since the CUB-200-2011 dataset is relatively small (approx. 30 images per class), training from scratch would cause overfitting. Using pre-trained ImageNet weights allows the model to already know basic shapes and textures.\n",
        "\n",
        "3. Data Augmentation: I have included RandomResizedCrop and RandomHorizontalFlip so that the model is robust to changes in scale and orientation, something critical in photos of birds in nature.\n",
        "\n",
        "\n",
        "4. Class Mapping: ImageFolder sorts classes alphabetically. Make sure that the order of subfolders matches the index (0-19) required by the Kaggle competition to avoid errors in the submission file.\n",
        "\n",
        "\n",
        "\n",
        "5. val_images Folder (Validation)\n",
        "It is used during the training process, at the end of each epoch (complete learning cycle).\n",
        "\n",
        "Purpose: It serves to evaluate how the model is learning in real time with data that was not used to adjust the weights.\n",
        "\n",
        "Technical utility: Detect Overfitting, if the accuracy on train increases but decreases on val, the model is memorizing instead of learning.\n",
        "\n",
        "Adjust Hyperparameters: It helps to decide whether to change the learning rate, the number of layers or the data augmentation technique.\n",
        "\n",
        "Save the best model: Normally, you only save the model weights if the accuracy in the validation folder improves compared to the previous epoch.\n",
        "\n",
        "6. test_images Folder (Test)\n",
        "It is used at the very end, once training has been completed.\n",
        "\n",
        "Purpose: To provide a final and impartial measure of model performance.\n",
        "\n",
        "\n",
        "This folder contains images that do not have a known label for you.\n",
        "\n",
        "It is the folder you process to generate the submission.csv file.\n",
        "\n",
        "\n",
        "7. A custom DataLoader was used to ensure that the order of images in the .csv file was consistent with their names,"
      ],
      "metadata": {
        "id": "GPvbgsNyY94-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mi6MQuU9ah0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture 2"
      ],
      "metadata": {
        "id": "5QamF39yuf0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "ujZk2VRMumWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "official_mapping = {\n",
        "    'Groove_billed_Ani': 0, 'Red_winged_Blackbird': 1, 'Rusty_Blackbird': 2,\n",
        "    'Gray_Catbird': 3, 'Brandt_Cormorant': 4, 'Eastern_Towhee': 5,\n",
        "    'Indigo_Bunting': 6, 'Brewer_Blackbird': 7, 'Painted_Bunting': 8,\n",
        "    'Bobolink': 9, 'Lazuli_Bunting': 10, 'Yellow_headed_Blackbird': 11,\n",
        "    'American_Crow': 12, 'Fish_Crow': 13, 'Brown_Creeper': 14,\n",
        "    'Yellow_billed_Cuckoo': 15, 'Yellow_breasted_Chat': 16,\n",
        "    'Black_billed_Cuckoo': 17, 'Gray_crowned_Rosy_Finch': 18,\n",
        "    'Bronzed_Cowbird': 19\n",
        "}"
      ],
      "metadata": {
        "id": "-WsNn43U0Ag6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths and Device Configuration\n",
        "BASE_DIR = '/content/drive/MyDrive/BDMA7_project_files'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_images')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_images')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test_images')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "Adf7ekXd1YTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aumento de Datos\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(0.2, 0.2, 0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Function to ensure correct mapping according to folder name\n",
        "def get_official_id(target_idx, dataset_class_to_idx):\n",
        "    folder_name = list(dataset_class_to_idx.keys())[list(dataset_class_to_idx.values()).index(target_idx)]\n",
        "    for bird_name, official_id in official_mapping.items():\n",
        "        if bird_name in folder_name:\n",
        "            return official_id\n",
        "    return target_idx\n",
        "\n",
        "\n",
        "# Loading Datasets\n",
        "temp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "internal_mapping = temp_ds.class_to_idx\n",
        "\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=data_transforms['train'],\n",
        "                                     target_transform=lambda y: get_official_id(y, internal_mapping))\n",
        "val_dataset = datasets.ImageFolder(VAL_DIR, transform=data_transforms['val_test'],\n",
        "                                   target_transform=lambda y: get_official_id(y, internal_mapping))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "XtMX_QyDxFkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ResNet-101 Model (Improved for >85% accuracy)\n",
        "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(num_ftrs, 20))\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-5},\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3}\n",
        "])\n",
        "\n",
        "# Scheduler (Reduce Learning rate if val loss doesn't decrease in 3 epochs)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "5XyHVidQyo1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(epochs=20):\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1} | Loss: {running_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Update Scheduler based on validation accuracy\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Start improved training\n",
        "train_and_validate(epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVPdd2Jy1yFG",
        "outputId": "ded17141-fa39-4b5b-ecc1-97fd2295af74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 2.6976 | Val Acc: 49.51%\n",
            "Epoch 2 | Loss: 1.9355 | Val Acc: 70.87%\n",
            "Epoch 3 | Loss: 1.3657 | Val Acc: 73.79%\n",
            "Epoch 4 | Loss: 1.0806 | Val Acc: 77.67%\n",
            "Epoch 5 | Loss: 0.9233 | Val Acc: 80.58%\n",
            "Epoch 6 | Loss: 0.7854 | Val Acc: 80.58%\n",
            "Epoch 7 | Loss: 0.7592 | Val Acc: 82.52%\n",
            "Epoch 8 | Loss: 0.7085 | Val Acc: 80.58%\n",
            "Epoch 9 | Loss: 0.6936 | Val Acc: 81.55%\n",
            "Epoch 10 | Loss: 0.7182 | Val Acc: 80.58%\n",
            "Epoch 11 | Loss: 0.6232 | Val Acc: 82.52%\n",
            "Epoch 12 | Loss: 0.6033 | Val Acc: 80.58%\n",
            "Epoch 13 | Loss: 0.5857 | Val Acc: 82.52%\n",
            "Epoch 14 | Loss: 0.5942 | Val Acc: 83.50%\n",
            "Epoch 15 | Loss: 0.5673 | Val Acc: 82.52%\n",
            "Epoch 16 | Loss: 0.5706 | Val Acc: 81.55%\n",
            "Epoch 17 | Loss: 0.5648 | Val Acc: 81.55%\n",
            "Epoch 18 | Loss: 0.5954 | Val Acc: 84.47%\n",
            "Epoch 19 | Loss: 0.5560 | Val Acc: 82.52%\n",
            "Epoch 20 | Loss: 0.5255 | Val Acc: 84.47%\n",
            "Epoch 21 | Loss: 0.5844 | Val Acc: 82.52%\n",
            "Epoch 22 | Loss: 0.5848 | Val Acc: 80.58%\n",
            "Epoch 23 | Loss: 0.5216 | Val Acc: 83.50%\n",
            "Epoch 24 | Loss: 0.5801 | Val Acc: 82.52%\n",
            "Epoch 25 | Loss: 0.5480 | Val Acc: 83.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv generation for kaggle\n",
        "def generate_submission(model, test_dir, output_name='sample_submission.csv'):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    # Special Test Dataset that gives us the file name\n",
        "    test_ds = datasets.ImageFolder(test_dir, transform=data_transforms['val_test'])\n",
        "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "    print(\"Starting prediction generation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # We get the actual file name\n",
        "            img_path, _ = test_ds.samples[i]\n",
        "            img_name = os.path.basename(img_path)\n",
        "\n",
        "            results.append({\n",
        "                'id': img_name,\n",
        "                'label': preds.item()\n",
        "            })\n",
        "\n",
        "    # Create DataFrame and export\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_name, index=False)\n",
        "    print(f\"File '{output_name}' generated correctly for Kaggle.\")\n"
      ],
      "metadata": {
        "id": "k5RuEwra161v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_submission(model, TEST_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzBwYmPu2KSr",
        "outputId": "fc0c1a00-9f03-424a-967c-82f9c270ec0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting prediction generation...\n",
            "File 'sample_submission.csv' generated correctly for Kaggle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HgQMqrv6s9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6eaFaMnZU8wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture 3"
      ],
      "metadata": {
        "id": "S_54Lh7RU8-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "o1tOEop-VkoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/BDMA7_project_files'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_images')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_images')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test_images')"
      ],
      "metadata": {
        "id": "HRi2FY2nVl-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "official_mapping = {\n",
        "    'Groove_billed_Ani': 0, 'Red_winged_Blackbird': 1, 'Rusty_Blackbird': 2,\n",
        "    'Gray_Catbird': 3, 'Brandt_Cormorant': 4, 'Eastern_Towhee': 5,\n",
        "    'Indigo_Bunting': 6, 'Brewer_Blackbird': 7, 'Painted_Bunting': 8,\n",
        "    'Bobolink': 9, 'Lazuli_Bunting': 10, 'Yellow_headed_Blackbird': 11,\n",
        "    'American_Crow': 12, 'Fish_Crow': 13, 'Brown_Creeper': 14,\n",
        "    'Yellow_billed_Cuckoo': 15, 'Yellow_breasted_Chat': 16,\n",
        "    'Black_billed_Cuckoo': 17, 'Gray_crowned_Rosy_Finch': 18,\n",
        "    'Bronzed_Cowbird': 19\n",
        "}"
      ],
      "metadata": {
        "id": "S-AegjVxVobM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformations de Alta Resolución (384px)\n",
        "# Increasing resolution is key to distinguishing similar species\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(384),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.3, 0.3, 0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(410),\n",
        "        transforms.CenterCrop(384),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Data Loading with Label Correction\n",
        "temp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "internal_mapping = temp_ds.class_to_idx\n",
        "\n",
        "def get_official_id(target_idx):\n",
        "    folder_name = list(internal_mapping.keys())[list(internal_mapping.values()).index(target_idx)]\n",
        "    for bird_name, official_id in official_mapping.items():\n",
        "        if bird_name in folder_name:\n",
        "            return official_id\n",
        "    return target_idx\n",
        "\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=data_transforms['train'], target_transform=get_official_id)\n",
        "val_dataset = datasets.ImageFolder(VAL_DIR, transform=data_transforms['val_test'], target_transform=get_official_id)\n",
        "\n",
        "# Batch size reduced to 16 due to increased resolution\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "# Model ConvNeXt-Tiny\n",
        "model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.DEFAULT)\n",
        "n_inputs = model.classifier[2].in_features\n",
        "model.classifier[2] = nn.Sequential(\n",
        "    nn.LayerNorm((n_inputs,), eps=1e-06, elementwise_affine=True),\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(n_inputs, 20)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# AdamW Optimizer and Scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "def train_model(epochs=15):\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / len(val_dataset)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train_loader):.4f} | Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_convnext.pth')\n",
        "\n",
        "# CSV Generation\n",
        "def generate_submission():\n",
        "    model.load_state_dict(torch.load('best_convnext.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    test_ds = datasets.ImageFolder(TEST_DIR, transform=data_transforms['val_test'])\n",
        "    results = []\n",
        "\n",
        "    print(\"Generating predictions with TTA...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (path, _) in enumerate(test_ds.samples):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img_name = os.path.basename(path)\n",
        "\n",
        "            # TTA: Original + Mirrored\n",
        "            t1 = data_transforms['val_test'](img).unsqueeze(0).to(device)\n",
        "            t2 = data_transforms['val_test'](transforms.functional.hflip(img)).unsqueeze(0).to(device)\n",
        "\n",
        "            out = (torch.softmax(model(t1), dim=1) + torch.softmax(model(t2), dim=1)) / 2\n",
        "            _, pred = torch.max(out, 1)\n",
        "\n",
        "            results.append({'id': img_name, 'label': pred.item()})\n",
        "\n",
        "    pd.DataFrame(results).to_csv('sample_submission.csv', index=False)\n",
        "    print(\"CSV generated\")\n"
      ],
      "metadata": {
        "id": "YHY6mognU-qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4cf8b2-9a25-4c1c-da53-35916dfe4824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 109M/109M [00:00<00:00, 174MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G74RopFVVN-",
        "outputId": "f20073c3-af95-4d7d-9582-eb35ad30f52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Loss: 2.3501 | Acc: 74.76%\n",
            "Epoch 2/20 | Loss: 0.9827 | Acc: 85.44%\n",
            "Epoch 3/20 | Loss: 0.6708 | Acc: 89.32%\n",
            "Epoch 4/20 | Loss: 0.5149 | Acc: 92.23%\n",
            "Epoch 5/20 | Loss: 0.4803 | Acc: 87.38%\n",
            "Epoch 6/20 | Loss: 0.4306 | Acc: 93.20%\n",
            "Epoch 7/20 | Loss: 0.3885 | Acc: 91.26%\n",
            "Epoch 8/20 | Loss: 0.3166 | Acc: 92.23%\n",
            "Epoch 9/20 | Loss: 0.3464 | Acc: 91.26%\n",
            "Epoch 10/20 | Loss: 0.2654 | Acc: 92.23%\n",
            "Epoch 11/20 | Loss: 0.2696 | Acc: 92.23%\n",
            "Epoch 12/20 | Loss: 0.2355 | Acc: 92.23%\n",
            "Epoch 13/20 | Loss: 0.1976 | Acc: 92.23%\n",
            "Epoch 14/20 | Loss: 0.2354 | Acc: 92.23%\n",
            "Epoch 15/20 | Loss: 0.2350 | Acc: 92.23%\n",
            "Epoch 16/20 | Loss: 0.2187 | Acc: 92.23%\n",
            "Epoch 17/20 | Loss: 0.1969 | Acc: 92.23%\n",
            "Epoch 18/20 | Loss: 0.2528 | Acc: 92.23%\n",
            "Epoch 19/20 | Loss: 0.2411 | Acc: 92.23%\n",
            "Epoch 20/20 | Loss: 0.2348 | Acc: 92.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_submission()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3dTcHB2VBM3",
        "outputId": "83f2e991-4bf1-4a69-b361-253625c1840f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions with TTA...\n",
            "CSV generated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5z6Pv4G0WMaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitectura 4"
      ],
      "metadata": {
        "id": "7f29sTlcMBly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "CPo3bCfGMExF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OFFICIAL_MAPPING = {\n",
        "    'Groove_billed_Ani': 0, 'Red_winged_Blackbird': 1, 'Rusty_Blackbird': 2,\n",
        "    'Gray_Catbird': 3, 'Brandt_Cormorant': 4, 'Eastern_Towhee': 5,\n",
        "    'Indigo_Bunting': 6, 'Brewer_Blackbird': 7, 'Painted_Bunting': 8,\n",
        "    'Bobolink': 9, 'Lazuli_Bunting': 10, 'Yellow_headed_Blackbird': 11,\n",
        "    'American_Crow': 12, 'Fish_Crow': 13, 'Brown_Creeper': 14,\n",
        "    'Yellow_billed_Cuckoo': 15, 'Yellow_breasted_Chat': 16,\n",
        "    'Black_billed_Cuckoo': 17, 'Gray_crowned_Rosy_Finch': 18,\n",
        "    'Bronzed_Cowbird': 19\n",
        "}"
      ],
      "metadata": {
        "id": "ZCOHE6jrMPxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/BDMA7_project_files'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_images')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_images')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test_images')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Augmentation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(0.3, 0.3, 0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# DATA LOADING WITH DYNAMIC MAPPING\n",
        "def get_target_transform(internal_mapping):\n",
        "    def transform(target_idx):\n",
        "        folder_name = list(internal_mapping.keys())[list(internal_mapping.values()).index(target_idx)]\n",
        "        for bird_name, official_id in OFFICIAL_MAPPING.items():\n",
        "            if bird_name in folder_name:\n",
        "                return official_id\n",
        "        return target_idx\n",
        "    return transform\n",
        "\n",
        "temp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "target_tf = get_target_transform(temp_ds.class_to_idx)\n",
        "\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=data_transforms['train'], target_transform=target_tf)\n",
        "val_dataset = datasets.ImageFolder(VAL_DIR, transform=data_transforms['val_test'], target_transform=target_tf)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# 4. MODEL DEFINITION FOR ENSEMBLE\n",
        "def get_resnet_model():\n",
        "    model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, 20))\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def get_effnet_model():\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, 20))\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "# GENERIC TRAINING FUNCTION\n",
        "def train_specific_model(model, save_name, epochs=20):\n",
        "    # We use Label Smoothing to improve generalization between similar species\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.2)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                _, preds = torch.max(model(inputs), 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / len(val_dataset)\n",
        "        print(f\"[{save_name}] Epoch {epoch+1} Val Acc: {val_acc:.2f}%\")\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), save_name)\n",
        "\n",
        "# GENERACIÓN DE ENSAMBLE + TTA\n",
        "def generate_submission():\n",
        "    # Load models entrenados\n",
        "    m1 = get_resnet_model()\n",
        "    m1.load_state_dict(torch.load('best_resnet.pth'))\n",
        "    m1.eval()\n",
        "\n",
        "    m2 = get_effnet_model()\n",
        "    m2.load_state_dict(torch.load('best_effnet.pth'))\n",
        "    m2.eval()\n",
        "\n",
        "    # Test dataset (without real labels)\n",
        "    test_ds = datasets.ImageFolder(TEST_DIR, transform=data_transforms['val_test'])\n",
        "    results = []\n",
        "\n",
        "    print(\"Starting Ensemble with Test Time Augmentation...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (path, _) in enumerate(test_ds.samples):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img_name = os.path.basename(path)\n",
        "\n",
        "            # TTA: Original + Horizontal Flip\n",
        "            t_orig = data_transforms['val_test'](img).unsqueeze(0).to(DEVICE)\n",
        "            t_flip = data_transforms['val_test'](transforms.functional.hflip(img)).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Average of probabilities from both models and both image versions\n",
        "            prob_m1 = (torch.softmax(m1(t_orig), dim=1) + torch.softmax(m1(t_flip), dim=1)) / 2\n",
        "            prob_m2 = (torch.softmax(m2(t_orig), dim=1) + torch.softmax(m2(t_flip), dim=1)) / 2\n",
        "\n",
        "            # Votación suave (promedio final)\n",
        "            final_probs = (prob_m1 + prob_m2) / 2\n",
        "            _, pred = torch.max(final_probs, 1)\n",
        "\n",
        "            results.append({'id': img_name, 'label': pred.item()})\n",
        "\n",
        "    pd.DataFrame(results).to_csv('sample_submission.csv', index=False)\n",
        "    print(\"Final file generated!\")\n"
      ],
      "metadata": {
        "id": "QSeEsus6MBBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar ResNet\n",
        "train_specific_model(get_resnet_model(), 'best_resnet.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF1if1OFMUGk",
        "outputId": "de099f81-fa1b-4959-f0cd-c675c1e76af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[best_resnet.pth] Epoch 1 Val Acc: 55.34%\n",
            "[best_resnet.pth] Epoch 2 Val Acc: 80.58%\n",
            "[best_resnet.pth] Epoch 3 Val Acc: 88.35%\n",
            "[best_resnet.pth] Epoch 4 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 5 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 6 Val Acc: 86.41%\n",
            "[best_resnet.pth] Epoch 7 Val Acc: 87.38%\n",
            "[best_resnet.pth] Epoch 8 Val Acc: 88.35%\n",
            "[best_resnet.pth] Epoch 9 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 10 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 11 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 12 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 13 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 14 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 15 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 16 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 17 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 18 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 19 Val Acc: 90.29%\n",
            "[best_resnet.pth] Epoch 20 Val Acc: 90.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar EfficientNet\n",
        "train_specific_model(get_effnet_model(), 'best_effnet.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlpqSCsjMbvh",
        "outputId": "fe99a827-f21b-4297-85fc-152038c2bf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[best_effnet.pth] Epoch 1 Val Acc: 25.24%\n",
            "[best_effnet.pth] Epoch 2 Val Acc: 52.43%\n",
            "[best_effnet.pth] Epoch 3 Val Acc: 66.02%\n",
            "[best_effnet.pth] Epoch 4 Val Acc: 72.82%\n",
            "[best_effnet.pth] Epoch 5 Val Acc: 81.55%\n",
            "[best_effnet.pth] Epoch 6 Val Acc: 84.47%\n",
            "[best_effnet.pth] Epoch 7 Val Acc: 85.44%\n",
            "[best_effnet.pth] Epoch 8 Val Acc: 85.44%\n",
            "[best_effnet.pth] Epoch 9 Val Acc: 84.47%\n",
            "[best_effnet.pth] Epoch 10 Val Acc: 87.38%\n",
            "[best_effnet.pth] Epoch 11 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 12 Val Acc: 86.41%\n",
            "[best_effnet.pth] Epoch 13 Val Acc: 85.44%\n",
            "[best_effnet.pth] Epoch 14 Val Acc: 87.38%\n",
            "[best_effnet.pth] Epoch 15 Val Acc: 87.38%\n",
            "[best_effnet.pth] Epoch 16 Val Acc: 86.41%\n",
            "[best_effnet.pth] Epoch 17 Val Acc: 85.44%\n",
            "[best_effnet.pth] Epoch 18 Val Acc: 86.41%\n",
            "[best_effnet.pth] Epoch 19 Val Acc: 87.38%\n",
            "[best_effnet.pth] Epoch 20 Val Acc: 87.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate CSV\n",
        "generate_submission()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWx0FcVMMcoR",
        "outputId": "4cad774d-cb64-4838-ec37-ca28e1af6b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ensamble con Test Time Augmentation...\n",
            "¡File final generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ARCHITECTURE 4 GAVE THE BEST RESULTS IN THE KAGGLE TEST"
      ],
      "metadata": {
        "id": "uzcHTpbIMpNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLdsyjOdqGxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitectura 5"
      ],
      "metadata": {
        "id": "ogzqr2tRqHAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "36iEevs7qLYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CONFIGURACIÓN Y MAPEO\n",
        "OFFICIAL_MAPPING = {\n",
        "    'Groove_billed_Ani': 0, 'Red_winged_Blackbird': 1, 'Rusty_Blackbird': 2,\n",
        "    'Gray_Catbird': 3, 'Brandt_Cormorant': 4, 'Eastern_Towhee': 5,\n",
        "    'Indigo_Bunting': 6, 'Brewer_Blackbird': 7, 'Painted_Bunting': 8,\n",
        "    'Bobolink': 9, 'Lazuli_Bunting': 10, 'Yellow_headed_Blackbird': 11,\n",
        "    'American_Crow': 12, 'Fish_Crow': 13, 'Brown_Creeper': 14,\n",
        "    'Yellow_billed_Cuckoo': 15, 'Yellow_breasted_Chat': 16,\n",
        "    'Black_billed_Cuckoo': 17, 'Gray_crowned_Rosy_Finch': 18,\n",
        "    'Bronzed_Cowbird': 19\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR = '/content/drive/MyDrive/BDMA7_project_files'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_images')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_images')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test_images')\n",
        "\n",
        "# MODELS\n",
        "def get_resnet():\n",
        "    m = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "    m.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(m.fc.in_features, 20))\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "def get_effnet():\n",
        "    m = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "    m.classifier[1] = nn.Sequential(nn.Dropout(0.5), nn.Linear(m.classifier[1].in_features, 20))\n",
        "    return m.to(DEVICE)\n",
        "\n",
        "# TRAINING\n",
        "def train_model_stage(model, stage_name, resolution, save_path, load_path=None, epochs=10):\n",
        "    print(f\"\\nIniciando {stage_name} a {resolution}px...\")\n",
        "\n",
        "    # Transformations según resolución\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(resolution),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Data loading\n",
        "    temp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "    internal_mapping = temp_ds.class_to_idx\n",
        "\n",
        "    def target_tf(target_idx):\n",
        "        folder_name = list(internal_mapping.keys())[list(internal_mapping.values()).index(target_idx)]\n",
        "        for name, idx in OFFICIAL_MAPPING.items():\n",
        "            if name in folder_name: return idx\n",
        "        return target_idx\n",
        "\n",
        "    train_ds = datasets.ImageFolder(TRAIN_DIR, transform=transform, target_transform=target_tf)\n",
        "    loader = DataLoader(train_ds, batch_size=8 if resolution > 300 else 32, shuffle=True)\n",
        "\n",
        "    # Load weights\n",
        "    if load_path and os.path.exists(load_path):\n",
        "        model.load_state_dict(torch.load(load_path))\n",
        "        print(f\"Weights loaded from {load_path}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4 if not load_path else 1e-5)\n",
        "\n",
        "    # Simple training loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1} completada.\")\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# 4. FLUJO COMPLETO AUTOMATIZADO\n",
        "def run_full_pipeline():\n",
        "    # MODEL 1: RESNET\n",
        "    res_model = get_resnet()\n",
        "    # Stage 1: 224px\n",
        "    train_model_stage(res_model, \"ResNet Stage 1\", 224, \"resnet_224.pth\", epochs=12)\n",
        "    # Stage 2: 448px (Load weights from 224)\n",
        "    train_model_stage(res_model, \"ResNet Stage 2\", 448, \"resnet_448.pth\", load_path=\"resnet_224.pth\", epochs=8)\n",
        "\n",
        "    # MODEL 2: EFFICIENTNET\n",
        "    eff_model = get_effnet()\n",
        "    # Stage 1: 224px\n",
        "    train_model_stage(eff_model, \"EffNet Stage 1\", 224, \"effnet_224.pth\", epochs=12)\n",
        "    # Stage 2: 448px\n",
        "    train_model_stage(eff_model, \"EffNet Stage 2\", 448, \"effnet_448.pth\", load_path=\"effnet_224.pth\", epochs=8)\n",
        "\n",
        "    print(\"\\nTraining completed\")\n",
        "\n",
        "# Execution\n",
        "run_full_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXH2dHEPrEuf",
        "outputId": "bb5df02f-733f-4729-d9e9-a8de15a699b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting ResNet Stage 1 a 224px...\n",
            "Epoch 1 completada.\n",
            "Epoch 2 completada.\n",
            "Epoch 3 completada.\n",
            "Epoch 4 completada.\n",
            "Epoch 5 completada.\n",
            "Epoch 6 completada.\n",
            "Epoch 7 completada.\n",
            "Epoch 8 completada.\n",
            "Epoch 9 completada.\n",
            "Epoch 10 completada.\n",
            "Epoch 11 completada.\n",
            "Epoch 12 completada.\n",
            "Model saved to resnet_224.pth\n",
            "\n",
            "Starting ResNet Stage 2 a 448px...\n",
            "Weights loadingdos desde resnet_224.pth\n",
            "Epoch 1 completada.\n",
            "Epoch 2 completada.\n",
            "Epoch 3 completada.\n",
            "Epoch 4 completada.\n",
            "Epoch 5 completada.\n",
            "Epoch 6 completada.\n",
            "Epoch 7 completada.\n",
            "Epoch 8 completada.\n",
            "Model saved to resnet_448.pth\n",
            "\n",
            "Starting EffNet Stage 1 a 224px...\n",
            "Epoch 1 completada.\n",
            "Epoch 2 completada.\n",
            "Epoch 3 completada.\n",
            "Epoch 4 completada.\n",
            "Epoch 5 completada.\n",
            "Epoch 6 completada.\n",
            "Epoch 7 completada.\n",
            "Epoch 8 completada.\n",
            "Epoch 9 completada.\n",
            "Epoch 10 completada.\n",
            "Epoch 11 completada.\n",
            "Epoch 12 completada.\n",
            "Model saved to effnet_224.pth\n",
            "\n",
            "Starting EffNet Stage 2 a 448px...\n",
            "Weights loadingdos desde effnet_224.pth\n",
            "Epoch 1 completada.\n",
            "Epoch 2 completada.\n",
            "Epoch 3 completada.\n",
            "Epoch 4 completada.\n",
            "Epoch 5 completada.\n",
            "Epoch 6 completada.\n",
            "Epoch 7 completada.\n",
            "Epoch 8 completada.\n",
            "Model saved to effnet_448.pth\n",
            "\n",
            "Training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  TRANSFORMACIÓN DE TEST (448px)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(512),\n",
        "    transforms.CenterCrop(448),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# PREDICTION FUNCTION WITH ENSEMBLE AND TTA\n",
        "def generate_submission(resnet_path, effnet_path, output_csv='sample_submission.csv'):\n",
        "    # Load models\n",
        "    print(\"Loading high resolution models...\")\n",
        "    m1 = get_resnet()\n",
        "    m1.load_state_dict(torch.load(resnet_path))\n",
        "    m1.eval()\n",
        "\n",
        "    m2 = get_effnet()\n",
        "    m2.load_state_dict(torch.load(effnet_path))\n",
        "    m2.eval()\n",
        "\n",
        "    # Test dataset\n",
        "    test_ds = datasets.ImageFolder(TEST_DIR, transform=test_transform)\n",
        "    results = []\n",
        "\n",
        "    print(f\"Generating predictions for {len(test_ds)} images...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (path, _) in enumerate(test_ds.samples):\n",
        "            img_name = os.path.basename(path)\n",
        "            img = Image.open(path).convert('RGB')\n",
        "\n",
        "            # TTA (Test Time Augmentation) ---\n",
        "            # Pasada 1: Original\n",
        "            img_orig = test_transform(img).unsqueeze(0).to(DEVICE)\n",
        "            # Pasada 2: Espejo Horizontal\n",
        "            img_flip = test_transform(transforms.functional.hflip(img)).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Obtener probabilidades (Softmax)\n",
        "            # Model 1 (ResNet)\n",
        "            p1_orig = torch.softmax(m1(img_orig), dim=1)\n",
        "            p1_flip = torch.softmax(m1(img_flip), dim=1)\n",
        "\n",
        "            # Model 2 (EffNet)\n",
        "            p2_orig = torch.softmax(m2(img_orig), dim=1)\n",
        "            p2_flip = torch.softmax(m2(img_flip), dim=1)\n",
        "\n",
        "            # AVERAGE OF 4 PREDICTIONS (Ensemble + TTA)\n",
        "            # Esto reduce el error aleatorio y mejora la confianza\n",
        "            avg_probs = (p1_orig + p1_flip + p2_orig + p2_flip) / 4\n",
        "\n",
        "            _, pred = torch.max(avg_probs, 1)\n",
        "            results.append({'id': img_name, 'label': pred.item()})\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f\"Processed {i+1} images...\")\n",
        "\n",
        "    # Create and save CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\nFile '{output_csv}' generated.\")\n"
      ],
      "metadata": {
        "id": "-pyhwulBv-US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_submission('resnet_448.pth', 'effnet_448.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odm4qVF8wAGC",
        "outputId": "f382d6e0-0dc0-4383-9209-33a8d4866642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading high resolution models...\n",
            "Generating predictions para 400 images...\n",
            "Processed 100 images...\n",
            "Processed 200 images...\n",
            "Processed 300 images...\n",
            "Processed 400 images...\n",
            "\n",
            "File 'sample_submission.csv' generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snfLY7hZ0-wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitectura 6"
      ],
      "metadata": {
        "id": "H7rAOoc8FcVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9O3UiMF4FfKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f2994d-35c7-4917-b9b8-8fffad7b6395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CONFIGURACIÓN Y MAPEO OFICIAL\n",
        "OFFICIAL_MAPPING = {\n",
        "    'Groove_billed_Ani': 0, 'Red_winged_Blackbird': 1, 'Rusty_Blackbird': 2,\n",
        "    'Gray_Catbird': 3, 'Brandt_Cormorant': 4, 'Eastern_Towhee': 5,\n",
        "    'Indigo_Bunting': 6, 'Brewer_Blackbird': 7, 'Painted_Bunting': 8,\n",
        "    'Bobolink': 9, 'Lazuli_Bunting': 10, 'Yellow_headed_Blackbird': 11,\n",
        "    'American_Crow': 12, 'Fish_Crow': 13, 'Brown_Creeper': 14,\n",
        "    'Yellow_billed_Cuckoo': 15, 'Yellow_breasted_Chat': 16,\n",
        "    'Black_billed_Cuckoo': 17, 'Gray_crowned_Rosy_Finch': 18,\n",
        "    'Bronzed_Cowbird': 19\n",
        "}"
      ],
      "metadata": {
        "id": "3Y8Ep7ZTFgip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/BDMA7_project_files'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_images')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_images')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test_images')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# TRANSFORMACIONES DE ALTA RESOLUCIÓN (448px)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(448),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(0.3, 0.3, 0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(512),\n",
        "        transforms.CenterCrop(448),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# DATA LOADING WITH DYNAMIC MAPPING\n",
        "def get_target_transform(internal_mapping):\n",
        "    def transform(target_idx):\n",
        "        folder_name = list(internal_mapping.keys())[list(internal_mapping.values()).index(target_idx)]\n",
        "        for bird_name, official_id in OFFICIAL_MAPPING.items():\n",
        "            if bird_name in folder_name:\n",
        "                return official_id\n",
        "        return target_idx\n",
        "    return transform\n",
        "\n",
        "temp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "target_tf = get_target_transform(temp_ds.class_to_idx)\n",
        "\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=data_transforms['train'], target_transform=target_tf)\n",
        "val_dataset = datasets.ImageFolder(VAL_DIR, transform=data_transforms['val_test'], target_transform=target_tf)\n",
        "\n",
        "# Batch size reduced to handle 448px images on GPU\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# MODELS\n",
        "def get_resnet_model():\n",
        "    model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, 20))\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def get_effnet_model():\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, 20))\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "# TRAINING CON MIXUP Y COSINE ANNEALING\n",
        "def train_specific_model(model, save_name, epochs=22):\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
        "\n",
        "    # Scheduler de Coseno para un descenso de LR más suave y efectivo\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # IMPLEMENTACIÓN DE MIXUP\n",
        "            # Combines two images and their labels to avoid overfitting\n",
        "            alpha = 0.2\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "\n",
        "            mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "            labels_a, labels_b = labels, labels[index]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(mixed_inputs)\n",
        "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluación\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                _, preds = torch.max(model(inputs), 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / len(val_dataset)\n",
        "        print(f\"[{save_name}] Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f} Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), save_name)\n",
        "\n",
        "# INFERENCIA CON ENSAMBLE Y TTA (Test Time Augmentation)\n",
        "def generate_submission():\n",
        "    m1 = get_resnet_model()\n",
        "    m1.load_state_dict(torch.load('best_resnet.pth'))\n",
        "    m1.eval()\n",
        "\n",
        "    m2 = get_effnet_model()\n",
        "    m2.load_state_dict(torch.load('best_effnet.pth'))\n",
        "    m2.eval()\n",
        "\n",
        "    test_ds = datasets.ImageFolder(TEST_DIR, transform=data_transforms['val_test'])\n",
        "    results = []\n",
        "\n",
        "    print(\"Starting Ensemble with High Resolution TTA...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (path, _) in enumerate(test_ds.samples):\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img_name = os.path.basename(path)\n",
        "\n",
        "            t_orig = data_transforms['val_test'](img).unsqueeze(0).to(DEVICE)\n",
        "            t_flip = data_transforms['val_test'](transforms.functional.hflip(img)).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Average of probabilities from both models and both image versions (4 predictions per foto)\n",
        "            prob_m1 = (torch.softmax(m1(t_orig), dim=1) + torch.softmax(m1(t_flip), dim=1)) / 2\n",
        "            prob_m2 = (torch.softmax(m2(t_orig), dim=1) + torch.softmax(m2(t_flip), dim=1)) / 2\n",
        "\n",
        "            final_probs = (prob_m1 + prob_m2) / 2\n",
        "            _, pred = torch.max(final_probs, 1)\n",
        "\n",
        "            results.append({'id': img_name, 'label': pred.item()})\n",
        "\n",
        "    pd.DataFrame(results).to_csv('sample_submission_0622.csv', index=False)\n",
        "    print(\"Final file generated!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QGr7KfEjFdxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_specific_model(get_resnet_model(), 'best_resnet.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iabZylOVFtPD",
        "outputId": "7047dbb7-311a-4452-cfcb-02c5c1a2301a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[best_resnet.pth] Epoch 1 Loss: 2.8864 Val Acc: 57.28%\n",
            "[best_resnet.pth] Epoch 2 Loss: 2.0023 Val Acc: 79.61%\n",
            "[best_resnet.pth] Epoch 3 Loss: 1.6298 Val Acc: 83.50%\n",
            "[best_resnet.pth] Epoch 4 Loss: 1.5032 Val Acc: 85.44%\n",
            "[best_resnet.pth] Epoch 5 Loss: 1.4884 Val Acc: 86.41%\n",
            "[best_resnet.pth] Epoch 6 Loss: 1.4777 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 7 Loss: 1.3994 Val Acc: 88.35%\n",
            "[best_resnet.pth] Epoch 8 Loss: 1.3018 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 9 Loss: 1.3882 Val Acc: 87.38%\n",
            "[best_resnet.pth] Epoch 10 Loss: 1.2745 Val Acc: 89.32%\n",
            "[best_resnet.pth] Epoch 11 Loss: 1.3147 Val Acc: 92.23%\n",
            "[best_resnet.pth] Epoch 12 Loss: 1.3383 Val Acc: 92.23%\n",
            "[best_resnet.pth] Epoch 13 Loss: 1.2933 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 14 Loss: 1.2664 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 15 Loss: 1.2642 Val Acc: 92.23%\n",
            "[best_resnet.pth] Epoch 16 Loss: 1.2212 Val Acc: 92.23%\n",
            "[best_resnet.pth] Epoch 17 Loss: 1.1591 Val Acc: 92.23%\n",
            "[best_resnet.pth] Epoch 18 Loss: 1.2110 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 19 Loss: 1.2105 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 20 Loss: 1.1502 Val Acc: 91.26%\n",
            "[best_resnet.pth] Epoch 21 Loss: 1.1630 Val Acc: 92.23%\n",
            "[best_resnet.pth] Epoch 22 Loss: 1.1343 Val Acc: 91.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_specific_model(get_effnet_model(), 'best_effnet.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IEkoHBoFv3h",
        "outputId": "c2557096-ee80-4b88-8118-55222d2fbb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[best_effnet.pth] Epoch 1 Loss: 2.9352 Val Acc: 50.49%\n",
            "[best_effnet.pth] Epoch 2 Loss: 2.6335 Val Acc: 64.08%\n",
            "[best_effnet.pth] Epoch 3 Loss: 2.2602 Val Acc: 71.84%\n",
            "[best_effnet.pth] Epoch 4 Loss: 2.0468 Val Acc: 81.55%\n",
            "[best_effnet.pth] Epoch 5 Loss: 1.8750 Val Acc: 83.50%\n",
            "[best_effnet.pth] Epoch 6 Loss: 1.8101 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 7 Loss: 1.6765 Val Acc: 87.38%\n",
            "[best_effnet.pth] Epoch 8 Loss: 1.6843 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 9 Loss: 1.5680 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 10 Loss: 1.5709 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 11 Loss: 1.5902 Val Acc: 87.38%\n",
            "[best_effnet.pth] Epoch 12 Loss: 1.4924 Val Acc: 89.32%\n",
            "[best_effnet.pth] Epoch 13 Loss: 1.4999 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 14 Loss: 1.5145 Val Acc: 89.32%\n",
            "[best_effnet.pth] Epoch 15 Loss: 1.4964 Val Acc: 89.32%\n",
            "[best_effnet.pth] Epoch 16 Loss: 1.4498 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 17 Loss: 1.4425 Val Acc: 90.29%\n",
            "[best_effnet.pth] Epoch 18 Loss: 1.4481 Val Acc: 89.32%\n",
            "[best_effnet.pth] Epoch 19 Loss: 1.4271 Val Acc: 89.32%\n",
            "[best_effnet.pth] Epoch 20 Loss: 1.4652 Val Acc: 88.35%\n",
            "[best_effnet.pth] Epoch 21 Loss: 1.4366 Val Acc: 89.32%\n",
            "[best_effnet.pth] Epoch 22 Loss: 1.4877 Val Acc: 89.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_submission()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDDAE959Fv6_",
        "outputId": "cf17f3c9-10ca-4d0d-d8f7-7a46e8d2e1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ensamble con TTA en Alta Resolución...\n",
            "¡File final generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def save_to_drive(source_path, destination_dir, filename='sample_submission_0622.csv'):\n",
        "    \"\"\"Saves a file from local path to Google Drive.\"\"\"\n",
        "    drive_path = os.path.join(destination_dir, filename)\n",
        "    try:\n",
        "        shutil.copy(source_path, drive_path)\n",
        "        print(f\"File '{filename}' saved to Drive: {drive_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Source file '{source_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to Drive: {e}\")\n",
        "\n",
        "# Save the generated sample_submission.csv\n",
        "save_to_drive('sample_submission_0622.csv', BASE_DIR)\n"
      ],
      "metadata": {
        "id": "TBixN8nLGIPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfa3c2f-50e3-4c8a-fcf9-6488f58dbed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'sample_submission_0622.csv' saved to Drive: /content/drive/MyDrive/BDMA7_project_files/sample_submission_0622.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchview\n",
        "from torchview import draw_graph\n",
        "import torchvision.models as models\n",
        "\n",
        "# input_size follows the format (batch_size, channels, height, width)\n",
        "model_graph = draw_graph(get_resnet_model(), input_size=(32, 3, 224, 224), expand_nested=True)\n",
        "model_graph.visual_graph\n",
        "model_graph.visual_graph.render(filename='arquitectura6_22', format='png', cleanup=True)\n",
        "\n",
        "'arquitectura6_22.png'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "GyU8T1fwBpTq",
        "outputId": "0bc54298-2903-49f7-d5e6-b09586c689f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchview in /usr/local/lib/python3.12/dist-packages (0.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchview) (0.21)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arquitectura6_22.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "# Requires defining input size (batch_size, channels, height, width)\n",
        "summary(get_resnet_model())\n",
        "\n",
        "model_graph = draw_graph(get_effnet_model(), input_size=(32, 3, 224, 224), expand_nested=True)\n",
        "model_graph.visual_graph\n",
        "model_graph.visual_graph.render(filename='arquitectura6_22', format='png', cleanup=True)\n",
        "\n",
        "'arquitectura6_22.png'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qeCq4cUT67GU",
        "outputId": "ea22b8f7-d867-4d64-b12f-07852b527538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arquitectura6_22.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def save_image_to_drive(source_path, destination_dir, filename='arquitectura6_22.png'):\n",
        "    \"\"\"Saves an image file from local path to Google Drive.\"\"\"\n",
        "    drive_path = os.path.join(destination_dir, filename)\n",
        "    try:\n",
        "        shutil.copy(source_path, drive_path)\n",
        "        print(f\"File '{filename}' saved to Drive: {drive_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Source file '{source_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to Drive: {e}\")\n",
        "\n",
        "# Save the generated architecture image\n",
        "save_image_to_drive('arquitectura6_22.png', BASE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPMBhpLfEH94",
        "outputId": "74553897-d742-4ca5-80bc-ba06e4534477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'arquitectura6_22.png' saved to Drive: /content/drive/MyDrive/BDMA7_project_files/arquitectura6_22.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Architecture 7"
      ],
      "metadata": {
        "id": "AwXRU49dvdQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### High-Resolution ResNet-50 with Ensemble\n",
        "\n",
        "In this experiment, a super ensemble is constructed by combining three models:\n",
        "1. The trained ResNet-50 model\n",
        "2. ResNet-101 pretrained on ImageNet\n",
        "3. EfficientNet-B0 pretrained on ImageNet\n",
        "\n",
        "Test-Time Augmentation (TTA) is applied during inference by averaging predictions from the original and horizontally flipped images. Predictions from all three models are averaged to produce the final class probabilities, and the class with the highest probability is selected as\n"
      ],
      "metadata": {
        "id": "9XK_9gTJurir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "BASE_DIR = '/content/bdma07/BDMA7_project_files'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_images')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_images')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test_images')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "OYzAixoFu087"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OFFICIAL_MAPPING = {\n",
        "    'Groove_billed_Ani': 0, 'Red_winged_Blackbird': 1, 'Rusty_Blackbird': 2,\n",
        "    'Gray_Catbird': 3, 'Brandt_Cormorant': 4, 'Eastern_Towhee': 5,\n",
        "    'Indigo_Bunting': 6, 'Brewer_Blackbird': 7, 'Painted_Bunting': 8,\n",
        "    'Bobolink': 9, 'Lazuli_Bunting': 10, 'Yellow_headed_Blackbird': 11,\n",
        "    'American_Crow': 12, 'Fish_Crow': 13, 'Brown_Creeper': 14,\n",
        "    'Yellow_billed_Cuckoo': 15, 'Yellow_breasted_Chat': 16,\n",
        "    'Black_billed_Cuckoo': 17, 'Gray_crowned_Rosy_Finch': 18,\n",
        "    'Bronzed_Cowbird': 19\n",
        "}\n"
      ],
      "metadata": {
        "id": "_qRz0W_Xu2Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(448),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(0.3, 0.3, 0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(512),\n",
        "        transforms.CenterCrop(448),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ],
      "metadata": {
        "id": "674c2Jb9u3sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_transform(internal_mapping):\n",
        "    def transform(target_idx):\n",
        "        folder_name = list(internal_mapping.keys())[\n",
        "            list(internal_mapping.values()).index(target_idx)\n",
        "        ]\n",
        "        for bird_name, official_id in OFFICIAL_MAPPING.items():\n",
        "            if bird_name in folder_name:\n",
        "                return official_id\n",
        "        return target_idx\n",
        "    return transform\n",
        "\n",
        "temp_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "target_tf = get_target_transform(temp_ds.class_to_idx)\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    TRAIN_DIR,\n",
        "    transform=data_transforms['train'],\n",
        "    target_transform=target_tf\n",
        ")\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    VAL_DIR,\n",
        "    transform=data_transforms['val_test'],\n",
        "    target_transform=target_tf\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "hrlMjij7u5Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet101():\n",
        "    model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(model.fc.in_features, 20)\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def get_effnet():\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "    model.classifier[1] = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(model.classifier[1].in_features, 20)\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def get_resnet50():\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(model.fc.in_features, 20)\n",
        "    )\n",
        "    return model.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "YxprepzYu7dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, save_name, epochs=15):\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            alpha = 0.2\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "            index = torch.randperm(inputs.size(0)).to(DEVICE)\n",
        "\n",
        "            mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
        "            labels_a, labels_b = labels, labels[index]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(mixed_inputs)\n",
        "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                _, preds = torch.max(model(inputs), 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / len(val_dataset)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), save_name)\n",
        "\n",
        "    print(f\"{save_name} best val acc: {best_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "DbnfgJLSu855"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(get_resnet101(), 'resnet101.pth', epochs=12)\n",
        "train_model(get_effnet(), 'effnet_b0.pth', epochs=12)\n",
        "train_model(get_resnet50(), 'resnet50.pth', epochs=12)\n"
      ],
      "metadata": {
        "id": "_RYISjtXu-q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_submission():\n",
        "    m1 = get_resnet101()\n",
        "    m1.load_state_dict(torch.load('resnet101.pth'))\n",
        "    m1.eval()\n",
        "\n",
        "    m2 = get_effnet()\n",
        "    m2.load_state_dict(torch.load('effnet_b0.pth'))\n",
        "    m2.eval()\n",
        "\n",
        "    m3 = get_resnet50()\n",
        "    m3.load_state_dict(torch.load('resnet50.pth'))\n",
        "    m3.eval()\n",
        "\n",
        "    test_ds = datasets.ImageFolder(TEST_DIR, transform=data_transforms['val_test'])\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for path, _ in test_ds.samples:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img_name = os.path.basename(path)\n",
        "\n",
        "            t_orig = data_transforms['val_test'](img).unsqueeze(0).to(DEVICE)\n",
        "            t_flip = data_transforms['val_test'](\n",
        "                transforms.functional.hflip(img)\n",
        "            ).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            p1 = (torch.softmax(m1(t_orig), 1) + torch.softmax(m1(t_flip), 1)) / 2\n",
        "            p2 = (torch.softmax(m2(t_orig), 1) + torch.softmax(m2(t_flip), 1)) / 2\n",
        "            p3 = (torch.softmax(m3(t_orig), 1) + torch.softmax(m3(t_flip), 1)) / 2\n",
        "\n",
        "            avg_pred = (p1 + p2 + p3) / 3\n",
        "            _, pred = torch.max(avg_pred, 1)\n",
        "\n",
        "            results.append({'id': img_name, 'label': pred.item()})\n",
        "\n",
        "    pd.DataFrame(results).to_csv('submission_super_ensemble.csv', index=False)\n",
        "\n",
        "generate_submission()\n"
      ],
      "metadata": {
        "id": "jJ6N_vaOu_4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_sub = pd.read_csv('submission_super_ensemble.csv')\n",
        "sample_sub = pd.read_csv('sample_submission (1).csv')\n",
        "\n",
        "if 'id' in my_sub.columns:\n",
        "    my_sub = my_sub.rename(columns={'id': 'path', 'label': 'class_idx'})\n",
        "\n",
        "my_sub = my_sub.set_index('path')\n",
        "my_sub = my_sub.reindex(sample_sub['path'])\n",
        "my_sub = my_sub.reset_index()\n",
        "\n",
        "my_sub.to_csv('submission_opt_2.csv', index=False)\n",
        "print(\"Saved submission_opt_2.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UNYgUfEMvBYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}